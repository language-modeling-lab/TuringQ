{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Libraries"
      ],
      "metadata": {
        "id": "3p86Npb8k6L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl"
      ],
      "metadata": {
        "id": "knmUcYo7k96W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import Hugging Face libraries\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, setup_chat_format\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "M7Uuyy_0lAIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging Face Hub\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "BEsHxJFxlIkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation TuringQ"
      ],
      "metadata": {
        "id": "xD9fUPqxmnf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"llm-lab/TuringQ\")"
      ],
      "metadata": {
        "id": "yD8s7Sx7pZJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the desired model to be evaluated\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load the tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "def query_model(\n",
        "    user_message,\n",
        "    temperature=0.7,\n",
        "    max_length=1024\n",
        "):\n",
        "    \"\"\"\n",
        "    Query the model with a user message and return the generated response.\n",
        "\n",
        "    Args:\n",
        "        user_message (str): The message from the user.\n",
        "        temperature (float): Sampling temperature for text generation.\n",
        "        max_length (int): Maximum length of the generated response.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the prompt with Chain of Thoughts approach\n",
        "    prompt = \"\"\"\n",
        "    You are a knowledgeable AI assistant specialized in Theory of Computation and Complexity. You will be answering questions related to this domain.\n",
        "\n",
        "    To provide a clear and structured response, you will follow the Chain of Thoughts approach:\n",
        "\n",
        "    Chain of Thoughts:\n",
        "\n",
        "    1. Analyze the question and identify core concepts, algorithms or problems.\n",
        "    2. Build a step-by-step solution approach, stating assumptions, defining variables/notations, and listing intermediate steps.\n",
        "    3. For proofs or complex calculations, show work explicitly, using relevant theorems, lemmas, or properties.\n",
        "    4. For true/false statements, provide clear justification or counterexample.\n",
        "    5. Review your Chain of Thoughts for logical soundness and completeness.\n",
        "\n",
        "    Use clear and concise language, avoiding unnecessary jargon.\n",
        "\n",
        "    Question: \"\"\" + user_message + \"\"\"\n",
        "\n",
        "    Final Answer:\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    sequences = pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=temperature,\n",
        "        eos_token_id=terminators,\n",
        "        max_new_tokens=max_length,\n",
        "        return_full_text=False,\n",
        "        pad_token_id=pipeline.model.config.eos_token_id\n",
        "    )\n",
        "\n",
        "    answer = sequences[0]['generated_text']\n",
        "    print(answer)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "u_6sT_J2nyjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Evaluation"
      ],
      "metadata": {
        "id": "_6oC_SsUmqzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your desired model, based on experiments meta-llama/Meta-Llama-3-8B-Instruct is the best for evaluation\n",
        "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "# Load the tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "def query_model(\n",
        "    masked_answer,\n",
        "    masked_solution,\n",
        "    temperature=0.7,\n",
        "    max_length=2048\n",
        "):\n",
        "    \"\"\"\n",
        "    Query the model with a masked answer and solution, and return the generated score.\n",
        "\n",
        "    Args:\n",
        "        masked_answer (str): The answer to be evaluated.\n",
        "        masked_solution (str): The correct solution for comparison.\n",
        "        temperature (float): Sampling temperature for text generation.\n",
        "        max_length (int): Maximum length of the generated response.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated score from the model.\n",
        "    \"\"\"\n",
        "    prompt=\"\"\"\n",
        "    You are an automated grading system for evaluating answers in the field of theory of computation and complexity. Your task is to assign a score (1, 2, 3, or 4) to a given answer based on its correctness and alignment with the provided solution, following the rubrics outlined below.\n",
        "    Rubrics:\n",
        "    Level 4 (Excellent):\n",
        "    - Answer is completely correct and aligns perfectly with the provided solution.\n",
        "    - Proofs, descriptions, true/false justifications, and calculations match the solution with no errors or omissions.\n",
        "    - Demonstrates a comprehensive understanding of the concepts.\n",
        "    Level 3 (Good):\n",
        "    - Answer is mostly correct, with only minor deviations or omissions compared to the provided solution.\n",
        "    - Proofs, descriptions, justifications, and calculations are largely accurate but may have a few minor flaws\n",
        "    - Shows a strong grasp of the key concepts.\n",
        "    Level 2 (Flawed):\n",
        "    - Answer has some significant differences or incorrect elements compared to the provided solution.\n",
        "    - Proofs, descriptions, justifications, and calculations contain several errors or omissions, but the core approach is partially valid.\n",
        "    - Demonstrates a basic understanding of the concepts but lacks depth.\n",
        "    Level 1 (Poor):\n",
        "    - Answer deviates substantially from the provided solution.\n",
        "    - Proofs, descriptions, justifications, and calculations are mostly incorrect or entirely missing crucial components.\n",
        "    - Exhibits a lack of understanding of the fundamental concepts.\n",
        "    Please note that the length of the answer should not be a factor in determining the score. The focus should be solely on the correctness and alignment with the provided solution.\n",
        "    Given Answer: \"\"\" + masked_answer + \"\"\"\\n Solution: \"\"\"+ masked_solution + \"\"\" \\n Based on the rubrics and the provided solution, assign a score (1, 2, 3, or 4) to the given answer\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Define terminators for the generated text\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "    # Generate the response from the model\n",
        "    sequences = pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=temperature,\n",
        "        eos_token_id=terminators,\n",
        "        max_new_tokens=max_length,\n",
        "        return_full_text=False,\n",
        "        pad_token_id=pipeline.model.config.eos_token_id\n",
        "    )\n",
        "\n",
        "    answer = sequences[0]['generated_text']\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "pGhfKPaRmuZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetunning Process"
      ],
      "metadata": {
        "id": "j62L0WWdktX2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKtcprC6hq4o"
      },
      "outputs": [],
      "source": [
        "# Define model ID and compute data type\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "compute_dtype = torch.bfloat16\n",
        "\n",
        "# Configure BitsAndBytes for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True)\n",
        "\n",
        "# Load model configuration\n",
        "model_config = AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=1024\n",
        ")\n",
        "\n",
        "# Load model with quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load training data from TuringQ file\n",
        "train_df = pd.read_excel(\"training.xlsx\")\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# Load validation data from TuringQ file\n",
        "test_df = pd.read_excel(\"validation.xlsx\")\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "def format_qa_template(example):\n",
        "    question = example[\"Question\"]\n",
        "    answer = example[\"Answer\"]\n",
        "    return {\"text\": question, \"labels\": answer}\n",
        "\n",
        "# Apply formatting to train and validation datasets\n",
        "train_dataset = train_dataset.map(format_qa_template)\n",
        "test_dataset = test_dataset.map(format_qa_template)\n",
        "\n",
        "# Combine train and validation datasets\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "# Setup chat format for model and tokenizer\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure PEFT (Parameter-Efficient Fine-Tuning)\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=64,\n",
        "        lora_dropout=0.05,\n",
        "        r=4,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results_llama3_sft/\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        do_eval=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        per_device_eval_batch_size=4,\n",
        "        log_level=\"info\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        learning_rate=5e-6,\n",
        "        eval_steps=500,\n",
        "        max_steps=4000,\n",
        "        num_train_epochs=3,\n",
        "        warmup_steps=100,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "\n",
        ")\n",
        "\n",
        "# Initialize SFTTrainer with model, datasets, and training arguments\n",
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=800,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_arguments,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Llama3-8B-ft-TuringQ"
      ],
      "metadata": {
        "id": "x-PZ8NwIlsj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the base model\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "# 2. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# 3. Set up chat format\n",
        "base_model, tokenizer = setup_chat_format(base_model, tokenizer)\n",
        "\n",
        "\n",
        "# 2. Load the PEFT adapter\n",
        "peft_model_id = \"PardisSzah/PEFT_TuringQ_llama3_FT\"\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "\n",
        "\n",
        "# 4. Merge the base model and PEFT adapter\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# 5. Set the model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "udrj2pTzlwJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"lighteval/MATH\", split=\"test\")\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Calculate the fraction needed to get 500 samples\n",
        "sample_fraction = 500 / len(df)\n",
        "\n",
        "# Create a stratified sample\n",
        "sample, _ = train_test_split(df, stratify=df[['level', 'type']], train_size=sample_fraction, random_state=42)\n"
      ],
      "metadata": {
        "id": "Doo53fLPl-Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "def query_model(system_message, user_message, temperature=0.7, max_length=1024):\n",
        "    user_message = \"Question: \" + user_message + \" Answer:\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "    sequences = pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=temperature,\n",
        "        eos_token_id=terminators,\n",
        "        max_new_tokens=max_length,\n",
        "        return_full_text=False,\n",
        "        pad_token_id=pipeline.model.config.eos_token_id\n",
        "    )\n",
        "    answer = sequences[0]['generated_text']\n",
        "    return answer\n",
        "\n",
        "system_message = \"\"\"\n",
        "You are an AI assistant designed to answer math questions.\n",
        "Please provide a step-by-step solution to the problem.\n",
        "\"\"\"\n",
        "\n",
        "# Generate answers using Llama 3 8B model\n",
        "sample['llama3_peft_answer_math'] = ''\n",
        "for idx, row in tqdm(sample.iterrows(), total=len(sample)):\n",
        "    question = row['problem']\n",
        "    answer = query_model(system_message, question, temperature=0.1, max_length=600)\n",
        "    print(answer)\n",
        "    sample.at[idx, 'llama3_peft_answer_math'] = answer\n"
      ],
      "metadata": {
        "id": "NqF-pjhCmE48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}